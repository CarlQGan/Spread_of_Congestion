{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ebaaa02-6d17-4c19-bf95-e17ed8a7d6ff",
   "metadata": {},
   "source": [
    "# Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5241be40-684d-49d1-bc8b-3c8d473f303c",
   "metadata": {},
   "source": [
    "- [Imports and Data Preprocessing](#Imports-and-Data-Preprocessing)\n",
    "- [Congestion Calculation](#Congestion-Calculation)\n",
    "- [SIR Model Fitment](#SIR-Model-Fitment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f922e1-ced7-43ed-95ce-d9c649cf3680",
   "metadata": {},
   "source": [
    "# Imports and Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf289c8-3b35-4c7a-8057-a8a534617bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import copy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import contextily as cx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41c96d5-d53b-4155-8a31-dc1fda92b80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBAL VARIABLES/CONSTANTS\n",
    "\n",
    "# SQLite File Path formatter\n",
    "__SQLITE_PATH_FORMAT = \"<PATH_TO_DATABASE>.sqlite\"\n",
    "\n",
    "# sections.shp File Path\n",
    "__SECTION_SHP = \"<PATH_TO_SECTIONS_FILE>.shp\"\n",
    "\n",
    "# Number of experiments\n",
    "__NUM_EXP = 11\n",
    "\n",
    "# number of links: 4433\n",
    "\n",
    "# Output File Directory\n",
    "__OUTPUT = \"output/\"\n",
    "\n",
    "# SQL Query to be excecuted for different tables\n",
    "__SQL_EXTRACT_MISECT_QUERY = 'SELECT * FROM MISECT'\n",
    "__SQL_EXTRACT_MILANE_QUERY = 'SELECT * FROM MILANE'\n",
    "\n",
    "# Columns to extract from different tables\n",
    "__MISECT_COLUMNS = ['ent', 'eid', 'sid', 'flow_capacity', 'speed', 'travel', 'traveltime', 'density', 'flow', 'dtime']\n",
    "__MILANE_COLUMNS = ['ent', 'eid', 'sid', 'lane', 'flow', 'speed', 'density', 'input_flow']\n",
    "\n",
    "# Actual time for each time step\n",
    "__TIME_REAL = ['14:15', '14:30', '14:45', '15:00', '15:15', '15:30', '15:45', '16:00', '16:15', '16:30', '16:45', '17:00', '17:15', '17:30', '17:45', '18:00', '18:15', '18:30', '18:45', '19:00', '19:15', '19:30', '19:45', '20:00']\n",
    "\n",
    "\n",
    "# Scenario Names\n",
    "__SCENARIOS = [\n",
    "    \"0% Dynamic En-Route\",\n",
    "    \"10% Dynamic En-Route\",\n",
    "    \"20% Dynamic En-Route\",\n",
    "    \"30% Dynamic En-Route\",\n",
    "    \"40% Dynamic En-Route\",\n",
    "    \"50% Dynamic En-Route\",\n",
    "    \"60% Dynamic En-Route\",\n",
    "    \"70% Dynamic En-Route\",\n",
    "    \"80% Dynamic En-Route\",\n",
    "    \"90% Dynamic En-Route\",\n",
    "    \"100% Dynamic En-Route\"\n",
    "]\n",
    "__SCENARIO_PRECENTAGES = [\n",
    "    \"0%\",\n",
    "    \"10%\",\n",
    "    \"20%\",\n",
    "    \"30%\",\n",
    "    \"40%\",\n",
    "    \"50%\",\n",
    "    \"60%\",\n",
    "    \"70%\",\n",
    "    \"80%\",\n",
    "    \"90%\",\n",
    "    \"100%\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c2a2ae-1e74-4600-bae9-48894da14078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SQL connection to our SQLite database\n",
    "\n",
    "# A list of established connections to our databases\n",
    "con = []\n",
    "\n",
    "for i in range(__NUM_EXP):\n",
    "    con.append(sqlite3.connect(__SQLITE_PATH_FORMAT.format(number=i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f4dd47-b037-4ef7-9f49-33664d6ec3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run SQL query and convert SQL to DataFrame\n",
    "\n",
    "# List of dataframes extracted from each experiment\n",
    "df = []\n",
    "# df_milane = []\n",
    "for i in range(__NUM_EXP):\n",
    "    # Run SQL\n",
    "    query = pd.read_sql(__SQL_EXTRACT_MISECT_QUERY, con[i])\n",
    "    \n",
    "    # Convert SQL to DataFrame\n",
    "    dataframe = pd.DataFrame(query, columns = __MISECT_COLUMNS)\n",
    "    df.append(dataframe)\n",
    "    \n",
    "    # query = pd.read_sql(__SQL_EXTRACT_MILANE_QUERY, con[i])\n",
    "    # dataframe = pd.DataFrame(query, columns = __MILANE_COLUMNS)\n",
    "    # df_milane.append(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7a2f76-4479-4357-9e70-cc257bbd4b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the sections.shp shapefile\n",
    "sections = gpd.read_file(__SECTION_SHP)\n",
    "sections.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a2472f-bff6-476a-a9dc-44fe59a2cab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a deep copy of df as back up in order not to rerun the above cell\n",
    "df_copy = copy.deepcopy(df)\n",
    "# df_milane_copy = copy.deepcopy(df_milane)\n",
    "sections_copy = copy.deepcopy(sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e277ae-00f9-4ef2-955e-75dd15530066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore the sections file in case of modification\n",
    "sections = copy.deepcopy(sections_copy)\n",
    "sections = sections.rename(columns={'speed': 'speed_limit'})\n",
    "df = copy.deepcopy(df_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bf435a-1cc9-4fb7-bf83-a154df390a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the sections with missing average speed\n",
    "df_total = []\n",
    "df_local = []\n",
    "df_throu = []\n",
    "print([df[i].shape for i in range(__NUM_EXP)])\n",
    "for i in range(__NUM_EXP):\n",
    "    df[i] = df[i][df[i]['speed'] >= 0.0]\n",
    "    df_total.append(copy.deepcopy(df[i][df[i]['sid'] == 0]))\n",
    "    df_local.append(copy.deepcopy(df[i][df[i]['sid'] == 1]))\n",
    "    df_throu.append(copy.deepcopy(df[i][df[i]['sid'] == 2]))\n",
    "print([df[i].shape for i in range(__NUM_EXP)])\n",
    "print([df_total[i].shape for i in range(__NUM_EXP)])\n",
    "print([df_local[i].shape for i in range(__NUM_EXP)])\n",
    "print([df_throu[i].shape for i in range(__NUM_EXP)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cb2a2f-d098-463c-9a05-485ce33213c6",
   "metadata": {},
   "source": [
    "# Congestion Calculation\n",
    "- [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c21e67-9d49-4ccc-af02-0984f56e58f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_experiment(gdf_list, col_name):\n",
    "    \"\"\"\n",
    "    Plot all the curves of the data for a list of GeoDataFrames on the designated column.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    gdf_list --- a list of GeoDataFrame that contains the processed data \n",
    "                 merged from sections.shp and sqlite output from aimsun\n",
    "    col_name --- the name of the column to be plotted\n",
    "    \"\"\" \n",
    "    for i in range(len(gdf_list)):\n",
    "        plt.plot(np.array(gdf_list[i][col_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007a2215-d374-40b8-b396-d97df13844a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess dataframe to merge with sections\n",
    "\n",
    "group_cols = ['ent','eid']\n",
    "# identify the columns which we want to average; this could\n",
    "# equivalently be defined as list(df.columns[4:])\n",
    "metric_cols = ['flow_capacity']\n",
    "\n",
    "# create a new DataFrame with a MultiIndex consisting of the group_cols\n",
    "# and a column for the mean of each column in metric_cols\n",
    "aggs = []\n",
    "for i in range(__NUM_EXP):\n",
    "    aggs.append(df_total[i].groupby(group_cols)[metric_cols].mean())\n",
    "\n",
    "# 1. remove the metric_cols from df because we are going to replace them\n",
    "# with the means in aggs \n",
    "# 2. dedupe to leave only one row with each combination of group_cols\n",
    "# in df\n",
    "for i in range(__NUM_EXP):\n",
    "    # Step 1\n",
    "    df_total[i].drop(metric_cols, axis=1, inplace=True)\n",
    "    \n",
    "    # Step 2\n",
    "    # df[i].drop_duplicates(subset=group_cols, keep='last', inplace=True)\n",
    "\n",
    "# add the mean columns from aggs into df\n",
    "for i in range(__NUM_EXP):\n",
    "    df_total[i] = df_total[i].merge(right=aggs[i], right_index=True, left_on=group_cols, how='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc5119d-7b69-4f19-a68e-aa3cb1a636b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge datasets: sections and dataframe\n",
    "sections_cong = []\n",
    "\n",
    "for i in range(__NUM_EXP):\n",
    "    sections_cong.append(pd.merge(df_total[i], sections, how='left', left_on='eid', right_on='eid'))\n",
    "    \n",
    "for i in range(__NUM_EXP):\n",
    "    sections_cong[i] = sections_cong[i][['ent', 'eid', 'speed', 'speed_limit', 'geometry']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cc932b-9697-4396-9ee7-32b648090c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the merged sections into GeoDataFrame and drop null values\n",
    "gdf = []\n",
    "\n",
    "for i in range(__NUM_EXP):\n",
    "    gdf.append(gpd.GeoDataFrame(sections_cong[i], geometry='geometry'))\n",
    "    gdf[i]['speed'] = gdf[i]['speed'].dropna()\n",
    "    gdf[i]['length'] = gdf[i]['geometry'].length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa87cee9-4d84-459d-979c-b725d6be1ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold for a section to be considered congested\n",
    "rho = np.linspace(0.1, 0.9, 9)\n",
    "rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ac95b9-991b-482e-bf5c-9bb33cba8a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column for each section at each threshold,\n",
    "# congested = 1, else 0\n",
    "for threshold in rho:\n",
    "    for i in range(__NUM_EXP):\n",
    "        speed_ratio = gdf[i]['speed'] / gdf[i]['speed_limit']\n",
    "        gdf[i]['congested at rho = ' + str(round(threshold, 1))] = [int(r < threshold) for r in speed_ratio]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f972d0-bf0e-425b-9d03-b52c2a30d5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column for congestion weight at each threshold,\n",
    "# congested = 1, else 0\n",
    "for threshold in rho:\n",
    "    for i in range(__NUM_EXP):\n",
    "        weight = gdf[i]['length'] * gdf[i]['congested at rho = ' + str(round(threshold, 1))]\n",
    "        gdf[i]['weight at rho = ' + str(round(threshold, 1))] = weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7dd13e-1f28-480e-92c1-52312d9c1ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group each GeoDataFrame on timestep and aggregate by sum\n",
    "# Remove the first row as it is the average of the rest\n",
    "gdf_agg = []\n",
    "\n",
    "for i in range(__NUM_EXP):\n",
    "    gdf_agg.append(gdf[i].groupby('ent').agg(np.sum).iloc[1:, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cf1da9-db8e-4029-b6ed-ca9f85885304",
   "metadata": {},
   "outputs": [],
   "source": [
    "for threshold in rho:\n",
    "    for i in range(__NUM_EXP):\n",
    "        gdf_agg[i]['congestion ratio at rho = ' + str(round(threshold, 1))] = gdf_agg[i]['weight at rho = ' + str(round(threshold, 1))] / total_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0874de-1396-4d9f-b61b-4f28c323ce6c",
   "metadata": {},
   "source": [
    "# SIR Model Fitment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38f41ca-d1c8-4b31-be0c-3d91eb992544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This model is adapted from the following source:\n",
    "# Marisa Eisenberg (marisae@umich.edu)\n",
    "# Yu-Han Kao (kaoyh@umich.edu) -7-9-17\n",
    "# SIR model example for python 2.7, with the following author updated to python 3.10\n",
    "# Qianxin (Carl) Gan (<hidden>@berkeley.edu)\n",
    "\n",
    "#### Import all the packages ####\n",
    "import scipy.optimize as optimize\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import copy\n",
    "\n",
    "from scipy.integrate import odeint as ode\n",
    "from scipy.stats import poisson\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075cae5e-769a-4136-981b-1327aded6184",
   "metadata": {},
   "source": [
    "## Estimating $R_0$ using Congested Link Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffe43b2-af4e-421b-b4ab-beeffd9943da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model equations for the scaled SIR model\n",
    "\n",
    "def model(ini, time_step, params):\n",
    "    Y = np.zeros(3) #column vector for the state variables\n",
    "    X = ini\n",
    "    mu = 0\n",
    "    beta = params[0]\n",
    "    gamma = params[1]\n",
    "\n",
    "    Y[0] = mu - beta * X[0] * X[1] - mu * X[0] #S\n",
    "    Y[1] = beta * X[0] * X[1] - gamma * X[1] - mu * X[1] #I\n",
    "    Y[2] = gamma * X[1] - mu * X[2] #R\n",
    "\n",
    "    return Y\n",
    "\n",
    "def x0fcn(params, data):\n",
    "    S0 = 1.0 - (data[0] / params[2])\n",
    "    I0 = data[0] / params[2]\n",
    "    R0 = 0.0\n",
    "    X0 = [S0, I0, R0]\n",
    "\n",
    "    return X0\n",
    "\n",
    "\n",
    "def yfcn(res, params):\n",
    "    return res[:,1]*params[2]\n",
    "\n",
    "\n",
    "# Simplified FIM (Fisher information matirx) function for the SIR model\n",
    "\n",
    "def minifisher (times, params, data, delta = 0.001):\n",
    "    #params = np.array(params)\n",
    "    listX = []\n",
    "    params_1 = np.array(params)\n",
    "    params_2 = np.array(params)\n",
    "    for i in range(len(params)):\n",
    "        params_1[i] = params[i] * (1 + delta)\n",
    "        params_2[i]= params[i] * (1 - delta)\n",
    "\n",
    "        res_1 = ode(model, x0fcn(params_1, data), times, args=(params_1,))\n",
    "        res_2 = ode(model, x0fcn(params_2, data), times, args=(params_2,))\n",
    "        subX = (yfcn(res_1, params_1) - yfcn(res_2, params_2)) / (2 * delta * params[i])\n",
    "        listX.append(subX.tolist())\n",
    "    X = np.matrix(listX)\n",
    "    FIM = np.dot(X, X.transpose())\n",
    "    return FIM\n",
    "\n",
    "\n",
    "\n",
    "# cost function for the SIR model\n",
    "\n",
    "def NLL(params, data, times): #negative log likelihood\n",
    "    params = np.abs(params)\n",
    "    data = np.array(data)\n",
    "    res = ode(model, x0fcn(params, data), times, args=(params,))\n",
    "    y = yfcn(res, params)\n",
    "    # nll = sum(y) - sum(data * np.log(y))\n",
    "    # note this is a slightly shortened version--there's an additive constant term missing but it \n",
    "    # makes calculation faster and won't alter the threshold. Alternatively, can do:\n",
    "    nll = -sum(np.log(poisson.pmf(np.round(data), np.round(y)))) # the round is b/c Poisson is for (integer) count data\n",
    "    # this can also barf if data and y are too far apart because the dpois will be ~0, which makes the log angry\n",
    "\n",
    "    # ML using normally distributed measurement error (least squares)\n",
    "    # nll = -sum(np.log(norm.pdf(data,y,0.1*np.mean(data)))) # example WLS assuming sigma = 0.1*mean(data)\n",
    "    # nll = sum((y - data)**2)  # alternatively can do OLS but note this will mess with the thresholds \n",
    "    #                             for the profile! This version of OLS is off by a scaling factor from\n",
    "    #                             actual LL units.\n",
    "    return nll\n",
    "\n",
    "\n",
    "\n",
    "# Profile Likelihood Generator\n",
    "\n",
    "# Input definitions\n",
    "# params = starting parameters (all, including the one to be profiled)\n",
    "# profparam = index within params for the parameter to be profiled\n",
    "#   ---reminder to make this allow you to pass the name instead later on\n",
    "# costfun = cost function for the model - should include params, times, and data as arguments.\n",
    "#   Note costfun doesn't need to be specially set up for fixing the profiled parameter, \n",
    "#   it's just the regular function you would use to estimate all the parameters\n",
    "#   (it will get reworked to fix one of them inside ProfLike)\n",
    "# times, data = data set (times & values, or whatever makes sense)\n",
    "#   ---possibly change this so it's included in costfun and not a separate set of inputs? Hmm.\n",
    "# perrange = the percent/fraction range to profile the parameter over (default is 0.5)\n",
    "# numpoints = number of points to profile at in each direction (default is 10)\n",
    "\n",
    "# Output\n",
    "# A list with:\n",
    "#   - profparvals: the values of the profiled parameter that were used\n",
    "#   - fnvals: the cost function value at each profiled parameter value\n",
    "#   - convergence: the convergence value at each profiled parameter value\n",
    "#   - paramestvals: the estimates of the other parameters at each profiled parameter value\n",
    "\n",
    "def proflike(params, profindex, cost_func, times, data, perrange = 0.5, numpoints = 10):\n",
    "    profrangedown = np.linspace(params[profindex], params[profindex] * (1 - perrange), numpoints).tolist()\n",
    "    profrangeup = np.linspace(params[profindex], params[profindex] * (1 + perrange), numpoints).tolist()[1:] #skip the duplicated values\n",
    "    profrange = [profrangedown, profrangeup]\n",
    "    currvals = []\n",
    "    currparams = []\n",
    "    currflags = []\n",
    "\n",
    "    def profcost(fit_params, profparam, profindex, data, times, cost_func):\n",
    "        paramstest = fit_params.tolist()\n",
    "        paramstest.insert(profindex, profparam)\n",
    "        return cost_func(paramstest, data, times)\n",
    "\n",
    "    fit_params = params.tolist() #make a copy of params so we won't change the origianl list\n",
    "    fit_params.pop(profindex)\n",
    "    # print('Starting profile...')\n",
    "    for i in range(len(profrange)):\n",
    "        for j in profrange[i]:\n",
    "            # print(i, j)\n",
    "            optimizer = optimize.minimize(profcost, fit_params, args=(j, profindex, data, times, cost_func), method='Nelder-Mead')\n",
    "            fit_params = np.abs(optimizer.x).tolist() #save current fitted params as starting values for next round\n",
    "            #print optimizer.fun\n",
    "            currvals.append(optimizer.fun)\n",
    "            currflags.append(optimizer.success)\n",
    "            currparams.append(np.abs(optimizer.x).tolist())\n",
    "\n",
    "    # structure the return output\n",
    "    profrangedown.reverse()\n",
    "    out_profparam = profrangedown + profrangeup\n",
    "    temp_ind = list(range(len(profrangedown)))\n",
    "    temp_ind.reverse()\n",
    "    out_params = [currparams[i] for i in temp_ind] + currparams[len(profrangedown):]\n",
    "    out_fvals = [currvals[i] for i in temp_ind] + currvals[len(profrangedown):]\n",
    "    out_flags = [currflags[i] for i in temp_ind] + currflags[len(profrangedown):]\n",
    "    output = {'profparam': out_profparam, 'fitparam': np.array(out_params), 'fcnvals': out_fvals, 'convergence': out_flags}\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a83d6bc-2494-4c9a-a3d8-0132c3fdd140",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_params(data, beta=0.4, gamma=0.25, N=4433):\n",
    "    \"\"\"\n",
    "    Estimate the parameters for the SIR model\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data: list or np.array\n",
    "        congested link count at each timestep\n",
    "    times: np.array\n",
    "        array of time steps\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    result_estimations: dict\n",
    "        a dictionary of parameter name and estimated value pairs\n",
    "    \"\"\"\n",
    "    #### Set initial parameter values and initial states ####\n",
    "    times=np.linspace(0, len(data) - 1, len(data))\n",
    "    params = [beta, gamma, N]  # make sure all the params and inition states are float\n",
    "    paramnames = ['beta', 'gamma', 'k']\n",
    "    # ini = x0fcn(params, data)\n",
    "    \n",
    "    #### Simulate the model ####\n",
    "    # res = ode(model, ini, times, args=(params,))\n",
    "    \n",
    "    #### Parameter estimation ####\n",
    "    optimizer = optimize.minimize(NLL, params, args=(data, times), method='Nelder-Mead')\n",
    "    paramests = np.abs(optimizer.x)\n",
    "    \n",
    "    #### Calculate the simplified Fisher Information Matrix (FIM) ####\n",
    "    # FIM = minifisher(times, params, data, delta = 0.001)\n",
    "    \n",
    "    #### Generate profile likelihoods and confidence bounds ####\n",
    "    # threshold = stats.chi2.ppf(0.95,len(paramests)) / 2.0 + optimizer.fun\n",
    "    perrange = 0.25 #percent range for profile to run across\n",
    "\n",
    "    profiles = {}\n",
    "    result_estimations = {}\n",
    "    for i in range(len(paramests)):\n",
    "        profiles[paramnames[i]] = proflike(paramests, i, NLL, times, data, perrange=perrange)\n",
    "        paramnames_fit = [n for n in paramnames if n not in [paramnames[i]]]\n",
    "        paramests_fit = [v for v in paramests if v not in [paramests[i]]]\n",
    "        \n",
    "        assert len(paramnames_fit) == len(paramests_fit)\n",
    "        for i in range(len(paramnames_fit)):\n",
    "            result_estimations[paramnames_fit[i]] = paramests_fit[i]\n",
    "            \n",
    "    return result_estimations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c566f1-cfb4-4053-842e-9646e3309554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate R_0 for all scenarios\n",
    "# where each row corresponds to each experiment\n",
    "# and column corresponds to rho values\n",
    "beta_matrix = []\n",
    "gamma_matrix = []\n",
    "R_0_matrix = []\n",
    "\n",
    "for i in range(__NUM_EXP):\n",
    "    beta_array = []\n",
    "    gamma_array = []\n",
    "    R_0_array = []\n",
    "    \n",
    "    for threshold in rho:\n",
    "        congestion_count = gdf_agg[i]['congested at rho = ' + str(round(threshold, 1))].tolist()\n",
    "        estimated_parameters = estimate_params(congestion_count, N=4433)\n",
    "        \n",
    "        beta = estimated_parameters['beta']\n",
    "        gamma = estimated_parameters['gamma']\n",
    "        R_0 = beta / gamma\n",
    "        \n",
    "        beta_array.append(beta)\n",
    "        gamma_array.append(gamma)\n",
    "        R_0_array.append(R_0)\n",
    "    beta_matrix.append(beta_array)\n",
    "    gamma_matrix.append(gamma_array)\n",
    "    R_0_matrix.append(R_0_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0e898c-cd90-4145-b75a-d56137e5d2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = fig.add_subplot(111)\n",
    "# ax.set_prop_cycle('color', [cm(1.*i/NUM_COLORS) for i in range(NUM_COLORS)])\n",
    "ax.set_prop_cycle('color', plt.cm.tab20(np.linspace(0, 1, __NUM_EXP)))\n",
    "x = np.arange(0.1, 1.0, 0.1)\n",
    "for i in range(__NUM_EXP):\n",
    "    plt.plot(x, R_0_matrix[i])\n",
    "plt.legend(\n",
    "    __SCENARIO_PRECENTAGES,\n",
    "    title='Dynamic En-Route Percentage',\n",
    ")\n",
    "plt.xlabel(r'$\\rho$', fontsize=18)\n",
    "plt.ylabel(r'$R_0$', fontsize=18)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.title(r'$R_0$ for All Experiments', fontsize=20)\n",
    "plt.savefig(__OUTPUT + 'R0_for_all_experiments_N_4433.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a830f84-f033-44d9-8cdd-790ed7aa44fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results\n",
    "rho_num = 9\n",
    "plt.figure(figsize=(10, 6))\n",
    "x = np.arange(__NUM_EXP)\n",
    "y = np.array(R_0_matrix)[:, rho_num - 1]\n",
    "\n",
    "model3 = np.poly1d(np.polyfit(x, y, 3))\n",
    "polyline = np.linspace(0, 10, 50)\n",
    "plt.plot(polyline, model3(polyline), color='red', linewidth=3)\n",
    "\n",
    "plt.plot(x, y, 'k-o', color='blue')\n",
    "\n",
    "plt.ylabel(r'$R_0$', fontsize='20')\n",
    "default_x_ticks = range(len(__SCENARIO_PRECENTAGES))\n",
    "plt.xticks(default_x_ticks, __SCENARIO_PRECENTAGES, fontsize='16')\n",
    "plt.yticks(fontsize='16')\n",
    "plt.xlabel('Dynamic En-Route', fontsize='16')\n",
    "plt.title(fr'$R_0$ for All Scenarios when $\\rho$ = 0.{rho_num}', fontsize='24')\n",
    "plt.legend(['Polynomial Fit', 'Data Points'], fontsize='15')\n",
    "plt.savefig(__OUTPUT + f'R0_for_all_experiments_rho_{rho_num}_N_4433.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152b2e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit polynomial models up to degree 5\n",
    "model1 = np.poly1d(np.polyfit(x, y, 1))\n",
    "model2 = np.poly1d(np.polyfit(x, y, 2))\n",
    "model3 = np.poly1d(np.polyfit(x, y, 3))\n",
    "model4 = np.poly1d(np.polyfit(x, y, 4))\n",
    "# model5 = np.poly1d(np.polyfit(x, y, 5))\n",
    "\n",
    "#create scatterplot\n",
    "polyline = np.linspace(0, 10, 50)\n",
    "plt.scatter(x, y)\n",
    "\n",
    "#add fitted polynomial lines to scatterplot \n",
    "plt.plot(polyline, model1(polyline), color='green')\n",
    "plt.plot(polyline, model2(polyline), color='red')\n",
    "plt.plot(polyline, model3(polyline), color='purple')\n",
    "plt.plot(polyline, model4(polyline), color='blue')\n",
    "# plt.plot(polyline, model5(polyline), color='orange')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710d5cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define function to calculate adjusted r-squared\n",
    "def adjR(x, y, degree):\n",
    "    results = {}\n",
    "    coeffs = np.polyfit(x, y, degree)\n",
    "    p = np.poly1d(coeffs)\n",
    "    yhat = p(x)\n",
    "    ybar = np.sum(y)/len(y)\n",
    "    ssreg = np.sum((yhat-ybar)**2)\n",
    "    sstot = np.sum((y - ybar)**2)\n",
    "    results['r_squared'] = 1- (((1-(ssreg/sstot))*(len(y)-1))/(len(y)-degree-1))\n",
    "\n",
    "    return results\n",
    "\n",
    "#calculated adjusted R-squared of each model\n",
    "print(adjR(x, y, 1))\n",
    "print(adjR(x, y, 2))\n",
    "print(adjR(x, y, 3))\n",
    "print(adjR(x, y, 4))\n",
    "# adjR(x, y, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6927ad8a-45b4-4a26-a8f1-5af0ba3e43fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_matrix_df = pd.DataFrame(beta_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50720af2-3731-4f90-8842-33ba37acd6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_matrix_df = pd.DataFrame(gamma_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93a6d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "R_0_matrix_df = pd.DataFrame(R_0_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292553cd",
   "metadata": {},
   "source": [
    "# Plotting Fitted SIR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ea630f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.integrate import odeint\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6de317",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fremont_SIR(I0=1, beta=0.25, gamma=0.05, N=4433):\n",
    "    # Total population, N.\n",
    "    # Initial number of infected and recovered individuals, I0 and R0.\n",
    "    # rho = 0.9\n",
    "    R0 = 0\n",
    "    # Everyone else, S0, is susceptible to infection initially.\n",
    "    S0 = N - I0 - R0\n",
    "    # A grid of time points (in quarters of an hour)\n",
    "    t = list(range(24))\n",
    "\n",
    "    # The SIR model differential equations.\n",
    "    def deriv(y, t, N, beta, gamma):\n",
    "        S, I, R = y\n",
    "        dSdt = -beta * S * I / N\n",
    "        dIdt = beta * S * I / N - gamma * I\n",
    "        dRdt = gamma * I\n",
    "        return dSdt, dIdt, dRdt\n",
    "\n",
    "    # Initial conditions vector\n",
    "    y0 = S0, I0, R0\n",
    "    # Integrate the SIR equations over the time grid, t.\n",
    "    ret = odeint(deriv, y0, t, args=(N, beta, gamma))\n",
    "    S, I, R = ret.T\n",
    "\n",
    "    # Plot the data on three separate curves for S(t), I(t) and R(t)\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    ax.plot(t, np.log(S / N), 'b', alpha=0.5, lw=2, label='S(t)')\n",
    "    ax.plot(t, np.log(I / N), 'r', alpha=0.5, lw=2, label='I(t)')\n",
    "    ax.plot(t, np.log(R / N), 'g', alpha=0.5, lw=2, label='R(t)')\n",
    "    ax.set_ylabel('Log Percentage', fontsize=16)\n",
    "    # ax.set_xlabel('Time', fontsize=16)\n",
    "    # ax.set_yticklabels(ax.get_yticklabels(), fontsize=16)\n",
    "    ax.yaxis.set_tick_params(length=0)\n",
    "    ax.xaxis.set_tick_params(length=0)\n",
    "    ax.tick_params(axis='x', labelsize=16)\n",
    "    ax.tick_params(axis='y', labelsize=16)\n",
    "    ax.set_xlim(0, 23)\n",
    "    # ax.set_yscale(\"log\")\n",
    "    \n",
    "    ax.yaxis.set_ticks(np.arange(-4, 0.5, 1.0))\n",
    "    ax.set_xticklabels(np.array(__TIME_REAL)[::5], rotation=90, fontsize=16)\n",
    "    ax.set_yticklabels(['1e-4', '1e-3', '1e-2', '1e-1', '1'], fontsize=16)\n",
    "    print(ax.get_xticks())\n",
    "    \n",
    "    ax.grid(visible=True, which='major', c='w', lw=2, ls='-')\n",
    "    legend = ax.legend(fontsize=12)\n",
    "    legend.get_frame().set_alpha(0.5)\n",
    "    # ax.grid(visible=True, which='major', c='w', lw=2, ls='-')\n",
    "    plt.savefig(__OUTPUT + f'SIR_model_beta_{beta}_gamma_{gamma}_{src_num}_{rho_num}.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1d51c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_num = 9\n",
    "rho_num = 9\n",
    "plot_fremont_SIR(gdf_agg[src_num][f'congested at rho = 0.{rho_num}'].tolist()[0], beta=beta_matrix[src_num][rho_num - 1], gamma=gamma_matrix[src_num][rho_num - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd861bf-6e9f-4d9e-b26d-facd39c4da00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot beta and gamm estimated results with fixed rho\n",
    "plt.figure(figsize=(10, 6))\n",
    "x = np.arange(__NUM_EXP)\n",
    "plt.plot(x, np.array(gamma_matrix)[:, 8])\n",
    "# plt.xticks([\n",
    "#     \"0% SRC(All SUE)\",\n",
    "#     \"10% SRC\",\n",
    "#     \"20% SRC\",\n",
    "#     \"30% SRC\",\n",
    "#     \"40% SRC\",\n",
    "#     \"50% SRC\",\n",
    "#     \"60% SRC\",\n",
    "#     \"70% SRC\",\n",
    "#     \"80% SRC\",\n",
    "#     \"90% SRC\",\n",
    "#     \"100% All SRC (No SUE)\"\n",
    "# ])\n",
    "plt.xlabel(r'$\\rho$')\n",
    "plt.ylabel(r'$\\beta$')\n",
    "plt.title(r'$\\beta$ for All $\\rho$')\n",
    "# plt.savefig(__OUTPUT + 'beta_for_all_experiments.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91eb21d1-5563-4214-a053-e94917805dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results\n",
    "plt.figure(figsize=(10, 6))\n",
    "x = np.arange(0.1, 1.0, 0.1)\n",
    "\n",
    "for i in range(__NUM_EXP):\n",
    "    plt.plot(x, beta_matrix[i])\n",
    "plt.legend([\n",
    "    \"0% SRC(All SUE)\",\n",
    "    \"10% SRC\",\n",
    "    \"20% SRC\",\n",
    "    \"30% SRC\",\n",
    "    \"40% SRC\",\n",
    "    \"50% SRC\",\n",
    "    \"60% SRC\",\n",
    "    \"70% SRC\",\n",
    "    \"80% SRC\",\n",
    "    \"90% SRC\",\n",
    "    \"100% All SRC (No SUE)\"\n",
    "])\n",
    "plt.xlabel(r'$\\rho$')\n",
    "plt.ylabel(r'$\\beta$')\n",
    "plt.title(r'$\\beta$ for All $\\rho$')\n",
    "# plt.savefig(__OUTPUT + 'beta_for_all_experiments.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f65877-3c7e-4eca-9cba-7b35124b3be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results\n",
    "plt.figure(figsize=(10, 6))\n",
    "x = np.arange(0.1, 1.0, 0.1)\n",
    "plt.plot(x, np.mean(beta_matrix, axis=0))\n",
    "plt.plot(x, np.mean(gamma_matrix, axis=0))\n",
    "# for i in range(__NUM_EXP):\n",
    "#     plt.plot(x, np.mean(beta_matrix[i]))\n",
    "#     plt.plot(x, gamma_matrix[i])\n",
    "plt.legend([\n",
    "    r'$\\beta$',\n",
    "    r'$\\gamma$'\n",
    "])\n",
    "plt.xlabel(r'$\\rho$')\n",
    "plt.ylabel(r'$\\gamma$')\n",
    "plt.title(r'Estimated Parameters for All $\\rho$')\n",
    "plt.savefig(__OUTPUT + 'Estimated Parameters for All $\\rho$.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fremont]",
   "language": "python",
   "name": "conda-env-fremont-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
